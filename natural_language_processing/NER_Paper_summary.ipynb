{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0WsysfmQhfoQ",
        "FZQFQ9RZrYSY",
        "VkVjnVZpo6Ch",
        "j2y2KI8Yrz0x",
        "sA1Yt9boK_hI",
        "Ap2DF4oDLCQZ",
        "DlGWbYp-LGtz"
      ],
      "authorship_tag": "ABX9TyMGX9zP/M3RorZh65gFx2AR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_0piNu1ZHGo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Top ranked competitions in NER](#scrollTo=Xis6bg1iTcEz)\n",
        "\n",
        ">[paper 2022 - DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition](#scrollTo=0WsysfmQhfoQ)\n",
        "\n",
        ">>[Details](#scrollTo=LcEZSsRYdI8Z)\n",
        "\n",
        ">>[Related works](#scrollTo=CaQMdVGndIxS)\n",
        "\n",
        ">>[Useful information](#scrollTo=WPSAt0TodNnb)\n",
        "\n",
        ">[Paper 2021 - Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning](#scrollTo=9WeotJSaixDs)\n",
        "\n",
        ">[Paper 2020 - LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](#scrollTo=FZQFQ9RZrYSY)\n",
        "\n",
        ">>[main idea](#scrollTo=VkVjnVZpo6Ch)\n",
        "\n",
        ">>[Details](#scrollTo=1TmRTvglpCx8)\n",
        "\n",
        ">>[Other methods](#scrollTo=4smnOlzApE2b)\n",
        "\n",
        ">>[usefull information](#scrollTo=99v2OCNzpG3D)\n",
        "\n",
        ">[Paper 2020 - Named entity recognition as dependency parsing.](#scrollTo=j2y2KI8Yrz0x)\n",
        "\n",
        ">>[Main idea](#scrollTo=8PFglECcIMuN)\n",
        "\n",
        ">>[Details](#scrollTo=dW68r4kqIMrB)\n",
        "\n",
        ">>[Related works](#scrollTo=Rkk9wq2yIMnl)\n",
        "\n",
        ">>[Useful information](#scrollTo=yl7wU0dWIMiU)\n",
        "\n",
        ">[Paper: 2021 Automated Concatenation of Embeddings for Structured Prediction](#scrollTo=SygBp_0uK62f)\n",
        "\n",
        ">>[Main idea](#scrollTo=1mcNIBthK85f)\n",
        "\n",
        ">>[Details](#scrollTo=EbTUQ-srK85f)\n",
        "\n",
        ">>>[ACE model](#scrollTo=7NaWClsECdne)\n",
        "\n",
        ">>[Related works](#scrollTo=U1wcuNNnK85g)\n",
        "\n",
        ">>[Useful information](#scrollTo=rfZOUxW6K85g)\n",
        "\n",
        ">>>[Neural architecture search](#scrollTo=J5YJB7ELGAsW)\n",
        "\n",
        ">>>[Revolutionary search](#scrollTo=J5YJB7ELGAsW)\n",
        "\n",
        ">>>[Embedding](#scrollTo=J5YJB7ELGAsW)\n",
        "\n",
        ">>[References](#scrollTo=D5ok0Rxx8T26)\n",
        "\n",
        ">[Paper: 2021 Co-regularized LUKE: Learning from Noisy Labels for Entity-Centric Information Extraction](#scrollTo=N72_vpoRK9qV)\n",
        "\n",
        ">>[Main idea](#scrollTo=I_KpnMDqK_G-)\n",
        "\n",
        ">>[Details](#scrollTo=wi_q3fy_K_HV)\n",
        "\n",
        ">>[Related works](#scrollTo=xBLjnJPIK_HV)\n",
        "\n",
        ">>[Useful information](#scrollTo=bkrjbaoGK_HV)\n",
        "\n",
        ">[Paper](#scrollTo=sA1Yt9boK_hI)\n",
        "\n",
        ">>[Main idea](#scrollTo=XB9H9KFtLBdB)\n",
        "\n",
        ">>[Details](#scrollTo=AI_paNQYLBdC)\n",
        "\n",
        ">>[Related works](#scrollTo=lF0GNTBlLBdC)\n",
        "\n",
        ">>[Useful information](#scrollTo=S_Zjl_tCLBdC)\n",
        "\n",
        ">[Paper](#scrollTo=Ap2DF4oDLCQZ)\n",
        "\n",
        ">>[Main idea](#scrollTo=1rH2OChmLFBL)\n",
        "\n",
        ">>[Details](#scrollTo=rqsbqeReLFBL)\n",
        "\n",
        ">>[Related works](#scrollTo=1TPPRlQMLFBL)\n",
        "\n",
        ">>[Useful information](#scrollTo=MGFXoi3FLFBM)\n",
        "\n",
        ">[Paper](#scrollTo=DlGWbYp-LGtz)\n",
        "\n",
        ">>[Main idea](#scrollTo=Vii-poyULIX-)\n",
        "\n",
        ">>[Details](#scrollTo=B1nlNt7_LIYa)\n",
        "\n",
        ">>[Related works](#scrollTo=jUaXTCA4LIYa)\n",
        "\n",
        ">>[Useful information](#scrollTo=aOnR3D6qLIYb)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "NssyIVCYHSFf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gRFjxgihchS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top ranked competitions in NER"
      ],
      "metadata": {
        "id": "Xis6bg1iTcEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Named Entity Recognition (NER) on CoNLL 2003 (English)__ [link](https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003)\n",
        "1. ACE+document context: Automated Concatenation of Embeddings for Structured Prediction 2021\n",
        "2. LUKE 483M: LUKE-Deep Contextualized Entity Representations with Entity-aware Self-attention 2020\n",
        "3. Co-regularized LUKE: Learning from Noisy Labels for Entity-Centric Information Extraction 2021\n",
        "\n",
        "__Named Entity Recognition (NER) on Ontonotes v5 (English)__ [link](https://paperswithcode.com/sota/named-entity-recognition-ner-on-ontonotes-v5)\n",
        "\n",
        "1. BERT-MRC+DSC: Dice Loss for Data-imbalanced NLP Tasks\n",
        "2. PL-Marker: Packed Levitated Marker for Entity and Relation Extraction\n",
        "3. Baseline + BS:  Boundary Smoothing for Named Entity Recognition\n",
        "\n",
        "__Named Entity Recognition (NER) on WNUT 2017__ [link](https://paperswithcode.com/sota/named-entity-recognition-on-wnut-2017)\n",
        "1. CL-KL: Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning 2021\n",
        "2. BERT-CRF: Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning 2021\n",
        "3. BERT + RegLER: Regularization for Long Named Entity Recognition 2021\n",
        "\n",
        "__Named Entity Recognition (NER) on ACE 2005__ [link](https://paperswithcode.com/sota/named-entity-recognition-on-ace-2005)\n",
        "1. cross-sentence ALB: A Frustratingly Easy Approach for Entity and Relation Extraction 2020\n",
        "2. GoLLIE: GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction 2023\n",
        "3. PromptNER [RoBERTa-large]: PromptNER: Prompt Locating and Typing for Named Entity Recognition 2023\n",
        "\n",
        "__Named Entity Recognition (NER) on BC5CDR__ [link](https://paperswithcode.com/sota/named-entity-recognition-ner-on-bc5cdr)\n",
        "\n",
        "1. BIDER Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning\n",
        "2. CoNER Enhancing Label Consistency on Document-level Named Entity Recognition\n",
        "3. CL-L2 Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning\n",
        "\n",
        "__ [link]()"
      ],
      "metadata": {
        "id": "oWqcmQ6eTe-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h9Gox_C9r0gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# paper 2022 - DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition\n",
        "\n"
      ],
      "metadata": {
        "id": "0WsysfmQhfoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Main idea</h2>"
      ],
      "metadata": {
        "id": "PS92zVjYhk83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improve the NER performance by building a knowledge base system that provides related context information for the NER model.\n",
        "\n",
        "1. Building the local KB based on Wikipedia, which matches the indomain data of the shared task and is fast.\n",
        "2. Fine-tuning pretrained contextual embeddings, though a multi-stage fine-tuning model\n"
      ],
      "metadata": {
        "id": "9HDEqU08hvQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details"
      ],
      "metadata": {
        "id": "LcEZSsRYdI8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Model overview</h3>\n",
        "\n",
        "Steps:\n",
        "1. An $n$ token sequence feeds into _knowledge retrieval module_ as query\n",
        "2. KR-module retrieves top-k related paragraphs in KB\n",
        "3. Then KB-module concatenates the input sentence and the related paragraphs and feeds the concatenated sequence into the embeddings\n",
        "4. The output token representations of the input sentence are fed into a linear-chain _conditional random field_ (CRF) (Lafferty et al., 2001) layer. The CRF layer produces the label predictions.\n",
        "5. Given the label predictions of multiple NER models with different random seeds, the ensemble module uses a voting strategy to decide the final predictions of the sentence\n",
        "\n",
        "---\n",
        "<h3> Knowledge retrieval module</h3>\n",
        "\n",
        "This module retrieves relevant documents by using the input sentence as a\n",
        "query. These retrieved documents act as contexts and are fed into the NER module. To enhance the retrieval quality, an iterative retrieval approach incorporates predicted entities of NER models into the search query.\n",
        "\n",
        "__Knowledge Base Building__:\n",
        "1. Based on Wikipedia we can build local Wikipedia search engines to retrieve the relevant context of the input sentences for each language.\n",
        "2.  use ElasticSearch (ES) to index them.\n",
        "3. define the document in the local Wikipedia search engines with three fields:\n",
        "  - sentence: acts as a sentence-level full-text retrieval field\n",
        "  - paragraph: stores the contexts of the sentence\n",
        "  - title: indicates the core entity described by the wiki page and can be used as an entity-level retrieval field.\n",
        "\n",
        "__Sentence Retrieval__ is a rettrieval at the sentence level, which takes the input sentence as a query and retrieves the top-k documents on the sentence field.\n",
        "\n",
        "__Iterative Entity Retrieval__\n",
        "1.  consider the relevance of the entities in the sentence to the title field in the documents during retrieval.\n",
        "2.  concatenate the entities in the sentences with “|” and then retrieve them on the title field.\n",
        "  - during training/eval phase: utilize the ground-truth entities directly\n",
        "  - during test phase: first perform the sentence retrieval and then use the entity mentions predicted by the model for entity retrieval.\n",
        "\n",
        "__Context Processing__\n",
        "\n",
        "After top-k results from the KB are retrieved, the system post-processes the retrieved documents into the contexts of the input sentence. There are three options of utilizing the texts in the documents:\n",
        "1. use the matched paragraph,\n",
        "2. use the matched sentence,\n",
        "3. use the matched sentence but remove the wiki anchors.\n",
        "\n",
        "---\n",
        "<H3> NER module</h3>\n",
        "\n",
        "Employ \"XLM-R Large\" as embeddings.\n",
        "- Input = concatenate (input text + seperator token `</s>` +  retrieved contexts)\n",
        "- The embedding layer in the NER module encode the concatenated input sequence and output the corresponding token representations. This output is then fed into a linear-chain CRF layer to obtain the conditional probability.\n",
        "\n",
        "- chunk retrieved texts to avoid the amount of subtoken in the\n",
        "sequence exceeding the maximum subtoken length in XLM-R (512)\n",
        "\n",
        "\n",
        "---\n",
        "<h3> Enemble model</h3>\n",
        "\n",
        "Given predictions from m models with different random seeds, majority voting generates the final prediction.\n",
        "- the label sequences are converted into entity spans\n",
        "- module ranks all spans in the predictions by the number of votes in descending order and selects the spans with more than 50% votes into the final prediction (based on the method presented by Yamada et al. (2020)).\n",
        "- The spans with more votes are kept if the selected spans have overlaps and the longer spans are kept if the spans have the same votes.\n",
        "\n",
        "---\n",
        "<h3> Training</h3>\n",
        "\n",
        "__NER training__\n",
        "\n",
        "\n",
        "__Multi-stage Fine-tuning__\n",
        "- Multi-stage fine-tuning aims at transferring the parameters of fine-tuned embeddings in a model at an early stage into other models in the next stage. The approach stores the checkpoint of fine-tuned XLM-R embeddings at the early stage and uses it as the initialization of XLM-R embeddings for model training at the next stage.\n",
        "- When try different types of KB, we can utilize the checkpoints of multilingual models at the previous stage to train the monolingual and code-mixed models with new types of contexts without training new multilingual models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KFETVcqWdI2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related works"
      ],
      "metadata": {
        "id": "CaQMdVGndIxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some architectures used for NER\n",
        "- BERT\n",
        "- LUKE\n",
        "- XLM-R\n",
        "\n",
        "Employ embedding training models for utilizing docment level context improves the NER performace. The downside is, in case of the lack of context in a dataset means the embeddings cannot take advantage of long-range dependencies for entity disambiguation.\n",
        "- Yu et al., 2020;\n",
        "- Luoma and Pyysalo, 2020;\n",
        "- Yamada et al., 2020;\n",
        "- Wang et al., 2021a\n",
        "\n",
        "__Retrieval-augmented context__ is effective for NER tasks: Wang et al., 2021b. The external relevant contexts can provide auxiliary information for disambiguating complex named entities.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sho7Z6zndIrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful information"
      ],
      "metadata": {
        "id": "WPSAt0TodNnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__IDEA__ - Use Google search to retrieve external contexts of the input sentence: Wang et al. (2021b). The current paper employs the same strategy.\n",
        "\n",
        "__IDEA__:  fine-tuned embeddings with specific training data or in a larger model architecture to improve model performance.\n",
        "- Shi and Lee (2021): two-stage fine-tuning:\n",
        "  1. first trains a general multilingual Enhanced Universal Dependency (Bouma et al., 2021) parser and then\n",
        "  2. finetunes on each specific language separately\n",
        "\n",
        "- Wang et al. (2021a): train models through concatenating fine-tuned embeddings.\n",
        "\n",
        "__IDEA__ Ensemble model: LUKE"
      ],
      "metadata": {
        "id": "16dvmGvlfd_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0eG4cmNYdOTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper 2021 - Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning"
      ],
      "metadata": {
        "id": "9WeotJSaixDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Useful information</h2>"
      ],
      "metadata": {
        "id": "_HwipZ6tmnsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Employ pretrain models for NER__\n",
        "\n",
        "- ELMO (Peters et al., 2018 [Summary](#elmoner))\n",
        "- Flair (Akbik et al., 2018 [Summary](#flairner))\n",
        "- BERT (Devlin et al., 2019 [Summary](#bertner))\n",
        "\n",
        "<font color='red'>__SOTA NER MODELS__M/font>\n",
        "\n",
        "- LUKE (Yamada et al. 2020 [Summary](#lukener))\n",
        "- Utilize the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 (k (Yu et al., 2020; Yamada et al., 2020))\n",
        "\n",
        "__Improve NER accuracy__ by including document-level contexts of the target sentence in the input of contextual embeddings ( (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020)\n",
        "\n",
        "__NER model__\n",
        "- Employ CFR layer for NER task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019)\n",
        "\n",
        "__Cooperative Learning__ <font color = 'red'> read again</font>"
      ],
      "metadata": {
        "id": "IszVmUoIjnKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper 2020 - LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\n",
        "\n",
        "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. LUKE: Deep contextualized entity representations with entityaware self-attention. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6442-6454, Online. Association for Computational Linguistics.\n",
        "\n",
        "[Link](https://doi.org/10.18653/v1/2020.emnlp-main.523)"
      ],
      "metadata": {
        "id": "FZQFQ9RZrYSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main idea"
      ],
      "metadata": {
        "id": "VkVjnVZpo6Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Main ideas__ - propose pretrained contextualized representations of words and entitiepropose (LUKE)\n",
        "\n",
        "1. Based on BERT, treats words and entities as independent tokens, output contextualized representation fo them. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia.\n",
        "2. An entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores.\n",
        "\n",
        "They conduct experiments with 5 entinty related tasks:  entity typing, relation classification,\n",
        "NER, cloze-style QA, and extractive QA\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JTSFzhMppCDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details"
      ],
      "metadata": {
        "id": "1TmRTvglpCx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__LUKE__\n",
        "\n",
        "It is trained using a new pretraining task:\n",
        "- randomly masking entities by replacing them with [MASK] entities\n",
        "-  trains the model by predicting the originals of these masked entities.\n",
        "\n",
        "Bachbone: RoBERTa\n"
      ],
      "metadata": {
        "id": "P-Py0ZScpEN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other methods"
      ],
      "metadata": {
        "id": "4smnOlzApE2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Static entity representation__:  assign a fixed embedding to each entity in the KB\n",
        "- knowledge embeddings trained on knowledge graphs (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016)\n",
        "-  embeddings trained using textual contexts or\n",
        "descriptions of entities retrieved from a KB (Yamada et al., 2016, 2017; Cao et al., 2017; Ganea and Hofmann, 2017)\n",
        "\n",
        "__Contexualized word representation__:\n",
        "- using the word representations of CWRs for entity-rlated task (Zhang et al., 2019; Baldini Soares\n",
        "et al., 2019; Peters et al., 2019; Joshi et al., 2020; Wang et al., 2019b, 2020). Some of the model architecture based on this approach that employ randomly masking word spans instead of a single word:\n",
        "  - Span-BERT\n",
        "  - ALBERT\n",
        "  - BART\n",
        "  - T5\n",
        "\n",
        "__Enhance CWR by injecting knowledge__:\n",
        "- enhance CWRs using static entity embeddings separately learned from a KB: ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019).\n",
        "- trains a model to detect whether an entity name in text is replaced by another entity name of the same type: WKLM (Xiong et al., 2020)\n",
        "-  pretraining based on the MLM and a knowledge-embedding objective: KEPLER (Wang et al., 2019b)\n",
        "-  extends CWRs using neural adapters that inject factual and linguistic knowledge: K-Adapter (Wang et al., 2020)\n",
        "\n"
      ],
      "metadata": {
        "id": "FuUNBViWpGf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Input representation</h3>"
      ],
      "metadata": {
        "id": "hbRIzA4lzLZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input representation of a token is computed using the following three embeddings:\n",
        "1. token embedding\n",
        "2. postion embedding\n",
        "3. entity type embedding\n",
        "\n",
        "__Token embedding__\n",
        "- it represents corresponding token\n",
        "- for computational efficiency it is decomposed into two matrices\n",
        "\n",
        "__Position embedding__\n",
        "- it represents the postiion of a token in the sentence\n",
        "- if an entity contains multiple words, its position embedding is computed by averaging the embeddings of the corresponding positions\n",
        "\n",
        "__Entity type embedding__\n",
        "- it represent if the token is an entity\n",
        "\n",
        "---\n",
        "Input representation of\n",
        "- word: summing the token and position embeddings\n",
        "- entity: summing the token, position, and entity type embeddings\n",
        "\n",
        "---\n",
        "Insert [CLS] at the beginning, and [SEP] at the end of the last word.\n",
        "\n"
      ],
      "metadata": {
        "id": "QVE9K7W0zSn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 3.2 Entity-aware Self-attention</h3>\n"
      ],
      "metadata": {
        "id": "qXqU0Vza1flN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LUKE enhances the self-attention mechanism by introducing an entity-aware query mechanism that uses a different query matrix for each possible pair of token types of $x_i$ and $x_j$. Depending if either of the pair is a word or entity, there are four different attention scores (refer to the paper - page 4).\n",
        "\n",
        "This causes additional cost of computing gradients and updating the parameters of the additional query matrices at the training time.\n",
        "\n"
      ],
      "metadata": {
        "id": "NPcLOOfD1hBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Pre-training</h3>"
      ],
      "metadata": {
        "id": "604sF77u3jR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Employed the conventional MLM and a new pretraining task (that is an extension of the MLM) to learn entity representations.\n",
        "\n",
        " Treat Wiki hyperlink as entity, and train on large corpus of Wiki.\n",
        "\n",
        "During training, randomly mask a certain percentage of the entities by replacing them with special [MASK] entities1 and then train the model to predict the masked entities. Formally, the original entity corresponding to a masked entity is predicted by applying the softmax function over all entities in our vocabulary.\n",
        "\n",
        "$$m = layer\\_norm(GELU((W_hh_e + b_h))$$\n",
        "$$\\hat{y} = softmax(BTm + bo)$$\n",
        "\n",
        "Where token embedding $A=BU$\n",
        "\n",
        "\n",
        "Activation: GELU\n",
        "\n",
        "Loss function: the sum of MLM loss and cross-entropy loss on predicting the masked entities."
      ],
      "metadata": {
        "id": "eNi1YIiD3xTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Modeing details</h3>"
      ],
      "metadata": {
        "id": "K3AhX_UR5lg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Backbone__: RoBERTA (large)\n",
        "\n",
        "pretrained CWRs based on a bidirectional transformer and a variant of BERT:\n",
        "- 1024 hidden dim\n",
        "- 24 hidden layers\n",
        "- 64 attention heads\n",
        "- 16 self-attention masks\n",
        "\n",
        "Entity token embedding: 256\n",
        "\n",
        "Number f parameters: 483M (RoBERTa: 335M + entity embeddings: 128M)\n",
        "\n",
        "Tokenizer: RoBERTa's tokenizer\n",
        "- word vocab size = 30k\n",
        "- entity vocab size: 500k (it incude two special entities,\n",
        "i.e., [MASK] and [UNK])\n",
        "\n",
        "\n",
        "Train with mask 15% of all words and entities at random. If an entity does not exist in the vocabulary,  replace it with the [UNK] entity.\n",
        "\n"
      ],
      "metadata": {
        "id": "CcLBvGVZ5oVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## usefull information"
      ],
      "metadata": {
        "id": "99v2OCNzpG3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__NER datasets/benchmarking__\n",
        "- Open Entity (entity typing),\n",
        "- TACRED (relation classification),\n",
        "- CoNLL-2003 (named entity recognition),\n",
        "- ReCoRD (cloze-style question answering), and\n",
        "- SQuAD 1.1 (extractive question answering)\n",
        "\n",
        "NLP tasks that involve entities:\n",
        "- relation classification,\n",
        "- entity typing,\n",
        "- named entity\n",
        "- recognition (NER), and\n",
        "- question answering (QA)\n",
        "\n",
        "Two approaches for entity representation:\n",
        "1. Conventional entitiy representation: assign a fixed embedding vectors to entities, which stores information about the entity in a KB (Bordes et al., 2013; Trouillon et al., 2016; Yamada et al., 2016, 2017).\n",
        "\n",
        "2. Contexual word representation (based on transformers such as BERT or RoBERTa). While some present CWR via trnasfomers, including Zhang et al., 2019; Peters et al., 2019; Joshi et al., 2020, transformer is not suitabl for CWR task because:\n",
        "  - CWRs do not output the span-level representations of entities (theytypically need to learn how to compute such representations based on a downstream dataset that is typically small.)\n",
        "  - Many entity-related tasks, e.g., relation classification and QA, involve reasoning about the relationships between entities.  It is difficult to perform such reasoning between entities because many entities are split into multiple tokens in the model.\n",
        "  - The word-based pretraining task of CWRs is not suitable for learning the representations of entities because predicting a masked word given other words in the entity, e.g., predicting “Rings” given “The Lord of the [MASK]”, is easier than predicting the entire entity.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hn7ULH-jpI5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper 2020 - Named entity recognition as dependency parsing.\n",
        "\n",
        "Juntao Yu, Bernd Bohnet, and Massimo Poesio. 2020. Named entity recognition as dependency parsing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470-6476, Online. Association for Computational Linguistics.\n",
        "\n",
        "[Link](https://doi.org/10.18653/v1/2020.acl-main.577)"
      ],
      "metadata": {
        "id": "j2y2KI8Yrz0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main idea"
      ],
      "metadata": {
        "id": "8PFglECcIMuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details"
      ],
      "metadata": {
        "id": "dW68r4kqIMrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related works"
      ],
      "metadata": {
        "id": "Rkk9wq2yIMnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful information"
      ],
      "metadata": {
        "id": "yl7wU0dWIMiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper: 2021 Automated Concatenation of Embeddings for Structured Prediction"
      ],
      "metadata": {
        "id": "SygBp_0uK62f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main idea"
      ],
      "metadata": {
        "id": "1mcNIBthK85f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proposed an automated concatenation for the embedding method, and improve the word embedding by finding the best candidates for the concatenation.\n",
        "This is a RL approach, in which a controller alternately samples a concatenation of embedings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward.  \n",
        "\n",
        "__Idea__: an iterative search process, guided by a controller that based on its belief models the effectiveness of individual embedding candidates in consideration for a specific task.\n",
        "\n",
        "At each step, the controller samples a concatenation of embeddings according to the belief model and then feeds the concatenated word representations as inputs to a task model, which in turn is trained on the task dataset and returns the __model accuracy as a reward signal to update the belief model__.\n",
        "\n",
        "To solve the optimizatin, employ policy gradient algorithm (Williams, 1992; Sutton and Barto, 1992).\n",
        "\n",
        "To improve the efficiency of the search process, design a special reward function by accumulating all the rewards based on the transformation between the current concatenation and all previously sampled concatenation."
      ],
      "metadata": {
        "id": "52tdWrv-DNU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details"
      ],
      "metadata": {
        "id": "EbTUQ-srK85f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model performace could be improved by concatenating embedding. The challenges for this approach are:\n",
        "- finding the best candidates for concatenation\n",
        "- embedding concatenation heavily relies on the task in hand\n",
        "- selecting the best embedding methods to generate embedding cadidates\n",
        "\n",
        "__This paper proposes__ an automated concatenation of embeddings (ACE), via reinforcement approach.\n",
        "\n",
        "\n",
        "\n",
        "Pretrained contextualized embeddings are powerful word representation. The word representation could be improved by concatenating different types of embeddings.\n",
        "\n",
        "The proposed method is applicable for a structured predicting task such as NER.\n",
        "\n",
        "---\n",
        "### ACE model\n",
        "Two components interact with each other repeaditly:\n",
        "- task moddel : predicts the task output,\n",
        "- controller: searches for better embedding concatenation as the word representation for the task model to achieve higher accuracy.\n",
        "\n",
        "---\n",
        "__Process__\n",
        "\n",
        "Given an embedding concatenation generated from the controller, the task model is trained over the task data and returns a reward to the controller. The controller receives the reward to update its parameter and samples a new embedding concatenation for the task model.\n",
        "\n",
        "$Controller \\xrightarrow {\\text{embedding}} \\text{task model} \\xrightarrow {\\text{reward}} controller \\dots$\n",
        "\n",
        "\n",
        "__Task model__\n",
        "\n",
        "In this paper authors used sequence structure and graph structure as two examples for the output structure $y$.\n",
        "- sequence-structured outputs: BiLSTM-CRF model (Ma and Hovy, 2016; Lample et al., 2016)\n",
        "- graph-structured outputs: BiLSTM-Biaffine model (Dozat and Manning, 2017)\n",
        "\n",
        "__Search space design__\n",
        "\n",
        "Search space: a set of neural networks, each represented as a DAG. Nodes represent operation, and edges represent input or output.\n",
        "\n",
        "In ACE, each node reperesents an embedding candidate. Input is a text sqeucnce $x$, and output is embedding $v^l$. NOTE: there is no connection between node! (word representation = concatenated embeddings) --> lower space size\n",
        "\n",
        "__Concatenation options__: BERT example\n",
        "- Devlin et al. (2019) concatenated the last four layers as word features\n",
        "- Kondratyuk and Straka (2019) applied a weighted sum of all twelve layers\n",
        "- IN ACE, authors follow a typical usage of each embedding --> lower space size ($2^l -1$ possible combination of nodes).\n",
        "\n",
        "__NAS in ACE__\n",
        "\n",
        "In ACE, authors fixed the weight of pretrained embedding candidates except for the character embeddings. Instead of sharing the parameters of the embeddings, authors share the parameters of the task models at each step of search. Also,  instead of deciding whether each node exists in the\n",
        "graph, authors keep all nodes in the search space and add an additional mask operation, which decide which embedding is used for concatenation.\n",
        "\n",
        "Since the input V is applied to a linear layer in the BiLSTM layer, multiplying the mask with the embeddings is equivalent to directly concatenating the selected embeddings.\n",
        "\n",
        "\n",
        "__Search in the space__\n",
        "\n",
        "During search, the controller generates the embedding mask for the task model iteratively.\n",
        "\n",
        "\n",
        "__Training__\n",
        "\n",
        "Training the controller\n",
        "- initialize dictionary $D$ with concatenations and scores\n",
        "- at $t=1$: train tast model with all embedding candicates concatenated .\n",
        "- Iterate the following steps until max iteration $T$:\n",
        "  - sample a concatenation $a^t$ based on probability distribution: $$P^{control}_{l}(a_l;\\theta_l) = \\begin{cases} \\sigma (\\theta_l) & a_l = 1 \\\\  1-P^{control}_{l}(a_l = 1;\\theta_l) & a_l = 0\\end{cases}$$\n",
        "  - train task model with $a_l$, evaluate model on the development set and retrieve accuracy $R_t$.\n",
        "  \n",
        "      Word represetntation of ith word in a concatenation of L types of word embedding = $v_i = [v_i^1, \\dots , v_i^L]$\n",
        "\n",
        "      Train model: $ v_i = [v_i^1a_1, \\dots ,v_i^La_L ]$\n",
        "  - Given concatenation $a^t$, accuracy $R_t$, and dictionary $D$, compute the gradient of the controller, and update the parameters of controller:\n",
        "  $$\\nabla_\\theta J_t(\\theta) \\approx \\sum^L_{l=1} \\nabla_theta \\log P^{control}_l (a^t_l;\\theta _l)r^t_l  \n",
        "  \\\\ r^t = \\sum_{i=1}^{t-1} (R_t - R_i)\\gamma^{Hamm(a^t,a^i)-1}|a^t,a^i| \\\\ \\gamma\\in\\{0,1\\}$$\n",
        "  - add $a^t$ and %R_t$ to $D$ and set $t=t+1$.\n",
        "\n",
        "When sampling $a^t$, avoid selecting previous concatenation ($a^{t-1}$) and the all-zero vector.\n",
        "\n",
        "If $a^t$ is already in the $D$, compare $R_t$ and keep the higher value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7NaWClsECdne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related works"
      ],
      "metadata": {
        "id": "U1wcuNNnK85g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Contexual embedding__\n",
        "- ELMo employes pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers.\n",
        "- Flair, as a kind of contextualized character embeddings and achieved strong performance in sequence labeling tasks.\n",
        "- BERT encodes contextualized sub-word information by Transformers\n",
        "\n",
        "__Neural architecture search__ currently is an active area of research.\n",
        "\n",
        "Most useful approaches:\n",
        "- evolutionary algorithms\n",
        "- reinforcement learning\n",
        "\n",
        "Given an embedding concatenation generated from the controller, the task model is trained over the task data and returns a reward to the controller. The controller receives the reward to update its parameter and samples a new embedding concatenation for the task model."
      ],
      "metadata": {
        "id": "eaesr-pkKlvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful information"
      ],
      "metadata": {
        "id": "rfZOUxW6K85g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Improve model performance__ by employ word representation based on concatenation of multiple pre-trained contexual embeddings and traditional non-contexual embeddings: e (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b).\n",
        "\n",
        "---\n",
        "### Neural architecture search\n",
        "\n",
        "__Neural architecture search__ aims to find the best model architecture:\n",
        "- image classification (Real et al., 2019),\n",
        "- semantic segmentation (Liu et al., 2019a), and\n",
        "- object detection (Ghiasiet al., 2019).\n",
        "- In natural language processing,\n",
        "  - find better RNN structures (Zoph and Le, 2017; Pham et al., 2018b)\n",
        "  - find better transformer structures (So et al.,2019; Zhu et al., 2020)\n",
        "\n",
        "A crusial part of NAS is search space design.\n",
        "- initial approach was to search through hand-crafted architectures. This approach requires a lot of GPU-hours.\n",
        "- more recent approach is employ reinforcement learning. In this approach, agent’s actions are the generation of neural architectures and the action space is identical to the search space. Previous works in this area:\n",
        "  - employ RNN r (Zoph and Le, 2017; Zhong et al., 2018; Zoph et al., 2018)\n",
        "  - employ Markov decision process  (Baker et al., 2017)\n",
        "---\n",
        "### Revolutionary search\n",
        "\n",
        "__definition__:  The algorithm repeatedly generates new populations through recombination and mutation operations and selects survivors through competing among the population.\n",
        "\n",
        "__Previous works__\n",
        "- (Miller et al., 1989; Angeline et al., 1994; Stanley and Miikkulainen, 2002; Floreano et al., 2008; Jozefowicz et al., 2015).\n",
        "- more recent works:  Real et al. (2017), Liu et al. (2018a), Wistuba (2018) and Real et al. (2019) applied tournament selection (Goldberg and Deb, 1991) for the parent selection while Xie and Yuille (2017) keeps all parents. Suganuma et al. (2017) and Elsken et al. (2018) chose the best model while Real et al. (2019) chose several latest models as survivors.\n",
        "---\n",
        "### Embedding\n",
        "\n",
        "__Non-contexual embedding__\n",
        "- Word2Vec (Mikolov et al., 2013)\n",
        "- Glove (Pennington et al.,2014)\n",
        "- fastText (Bojanowski et al., 2017)\n",
        "\n",
        "__Character embedding__\n",
        "- Santos and Zadrozny, 2014\n",
        "\n",
        "__pretrained contextualized embeddings__\n",
        "- ELMo (Peters et al.,2018), a pretrained contextualized word embedding generated with multiple Bidirecttional LSTM layers,\n",
        "- Flair (Akbik et al., 2018), a kind of contextualized character embeddings,\n",
        "- BERT (Devlin et al., 2019),  encodes contextualized sub-word information by Transformeand, and\n",
        "- XLM-R (Conneau et al., 2020)\n",
        "\n",
        "---\n",
        "__NLP structured prediction tasks:__\n",
        "- NER\n",
        "- POS tagging\n",
        "- chunking\n",
        "- aspect extraction\n",
        "- synthatic dependency parsing (Tesnière 1959)\n",
        "- semantic dependency parsing  (Oepen et al., 2014)"
      ],
      "metadata": {
        "id": "J5YJB7ELGAsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "D5ok0Rxx8T26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 380–390, Atlanta, Georgia. Association for Computational Linguistics."
      ],
      "metadata": {
        "id": "TELDbj718WQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper: 2021 Co-regularized LUKE: Learning from Noisy Labels for Entity-Centric Information Extraction"
      ],
      "metadata": {
        "id": "N72_vpoRK9qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main idea\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I_KpnMDqK_G-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Idea__ Propose a general denoising framework\n",
        "- addressing the challenge of overfitting noisy labels, by proposing a noise robust model: co-regularization framework for entity-centric information extraction\n",
        "\n",
        "__How__\n",
        "1. Employ several neural models with identical structures but different parameter initialization\n",
        "2. Jointly optimize models with the task specific-losses and jointly regulazied to generate similar predictions based on an aggrement loss (KL-divergance). This approach prevents overfitting noisy labels.\n",
        "3. for instances where a classifier’s predictions disagree with labels, the agreement loss encourages the classifier to give similar predictions to the other classifier(s) instead of the actual (possibly noisy) labels\n",
        "\n",
        "\n",
        "__Motivation__\n",
        "- previuous studies shows noisy labels, compare with clean labels, require more training steps to be memorized and are more frequenly forgotten. This makes them identifiable during training: if multiple model disagree, there is a high possibility that they pick a noisy label.\n",
        "\n",
        "__Note__: proposed framework does not use external resources such as a\n",
        "clean development dataset (Qin et al., 2018)."
      ],
      "metadata": {
        "id": "TUKfZNHfzoDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details"
      ],
      "metadata": {
        "id": "wi_q3fy_K_HV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Implementation__\n",
        "- LUKE: employ the original hyperparameters\n",
        "- Adam\n",
        "- LR: 1e-5 (CoNLL2003); 6e-5 (TACRED)\n",
        "- LR decay 0\n",
        "- batch size 64\n",
        "- TACRED: 5spochs; CoNLL2003: 50 epochs\n",
        "- $\\gamma$: {1,2,5,10,20}\n",
        "- $\\alpha$: {10, 30, 50, 70, 90}\n",
        "- Two models and employ average method for aggregation\n",
        "\n",
        "__Observations__\n",
        "- increasing numbr of models from 2 to 4 for large model rewarded a marginal gain in performance, while in smaller model does not improve performance."
      ],
      "metadata": {
        "id": "RI7eb2j3EwHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related works"
      ],
      "metadata": {
        "id": "xBLjnJPIK_HV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__CrossWeigh (Wang et al., 2019c):__\n",
        "- approach/idea: denoises a natural language dataset without using extra learning resources. First train multiple independent models on different partitions of training data and then downweighs instances on which the models disagree. DOWNSIDE: requires high computational resources (redundant models).\n",
        "\n",
        "__Arpit et al., 2017; Toneva et al., 2019__ showed noisy labels often have delayed learning curves.\n",
        "\n",
        "---\n",
        "__Other approaches tackling noisy labeled dataset__\n",
        "- __CrossWeigh: a model for noisy-label (Wang et al., 2019c)__ - Idea/steps\n",
        "  \n",
        "  1. It partitions the training set into equalsized chunks, reserves each chunk, and then trains several models on the rest ones.\n",
        "  2. After training, the models predict on the reserved chunk, and instances on which the models disagree are downweighted.\n",
        "  3. In the end, the chunks are combined and used to train a new model for inference.\n",
        "- Small-loss selection (Jiang et al., 2018; Han et al., 2018; Lee and Chung, 2020)\n",
        "  - prunes the instances with the largest training losses in the training batches. - - This method is motivated by the fact that the noisy instances take a longer time to be memorized and usually cause a large training loss.\n",
        "- __Relabling__\n",
        "  - Instead of pruning the large-loss training instances, we relabel them with the most likely labels from model predictions."
      ],
      "metadata": {
        "id": "Y02sbKko60Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful information"
      ],
      "metadata": {
        "id": "bkrjbaoGK_HV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Formulate relation extraction task__\n",
        "-  Shi and Lin (2019), formulate this task as a sentence classification problem.\n",
        "- Steps:\n",
        "  1. first apply the entity masking technique (Zhang et al., 2017b) to the input\n",
        "sentence and replace the subject and object entities with their named entity types\n",
        "  2. then feed the sentence to the pre-trained language model and use a softmax classifier on the representation of the [CLS] token to predict the relation.\n",
        "\n",
        "__Formulate named entity task__:\n",
        "- Devlin et al. (2019), formulate the task as a token classification problem.\n",
        "- Steps:\n",
        "  1. A Transformer-based language model first tokenizes an input sentence into a sub-token sequence.\n",
        "  2. To classify each token, the representation of its first sub-token is sent into a softmax classifier.\n",
        "  3. Employ the BIO tagging scheme (Ramshaw and Marcus, 1995) and output the tag with the maximum likelihood as the predicted label."
      ],
      "metadata": {
        "id": "A8gsAuseBlaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper"
      ],
      "metadata": {
        "id": "sA1Yt9boK_hI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main idea"
      ],
      "metadata": {
        "id": "XB9H9KFtLBdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details"
      ],
      "metadata": {
        "id": "AI_paNQYLBdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related works"
      ],
      "metadata": {
        "id": "lF0GNTBlLBdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful information"
      ],
      "metadata": {
        "id": "S_Zjl_tCLBdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper"
      ],
      "metadata": {
        "id": "Ap2DF4oDLCQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main idea"
      ],
      "metadata": {
        "id": "1rH2OChmLFBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details"
      ],
      "metadata": {
        "id": "rqsbqeReLFBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related works"
      ],
      "metadata": {
        "id": "1TPPRlQMLFBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful information"
      ],
      "metadata": {
        "id": "MGFXoi3FLFBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper"
      ],
      "metadata": {
        "id": "DlGWbYp-LGtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main idea"
      ],
      "metadata": {
        "id": "Vii-poyULIX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details"
      ],
      "metadata": {
        "id": "B1nlNt7_LIYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related works"
      ],
      "metadata": {
        "id": "jUaXTCA4LIYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful information"
      ],
      "metadata": {
        "id": "aOnR3D6qLIYb"
      }
    }
  ]
}