# General idea

 Recovering the input prompt conditioned on the language model’s next-token probabilities. More specifically the proposed method predicts prompts by “unrolling” the distribution vector into a sequence that can be processed effectively by a pretrained encoder-decoder language model.
 

# Approach

## Other approaches
- Jailbreak
- LLM few-shot
- sample inverter
# Technical details

# Additional notes
